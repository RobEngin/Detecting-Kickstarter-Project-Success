group_by(launch_year) %>%
summarize(success_percentage = mean(state_bi == 1) * 100)
ggplot(success_rate, aes(x = launch_year, y = success_percentage)) +
geom_bar(stat="identity", fill="#2596be") +
ggtitle("Success rate by Year") +
xlab("Year") +
ylab("Success rate")
##############################
# PLOT: Success rate by Category #
success_rate <- data %>%
group_by(main_category) %>%
summarize(success_percentage = mean(state_bi == 1) * 100)
ggplot(success_rate, aes(x = main_category, y = success_percentage)) +
geom_bar(stat="identity", fill="#2596be") +
ggtitle("Success rate by Category") +
xlab("Category") +
ylab("Success rate")
##############################
# PLOT: Success rate by Country #
success_rate <- data %>%
group_by(country) %>%
summarize(success_percentage = mean(state_bi == 1) * 100)
ggplot(success_rate, aes(x = country, y = success_percentage)) +
geom_bar(stat="identity", fill="#2596be") +
ggtitle("Success rate by Country") +
xlab("Country") +
ylab("Success rate")
mean(success_rate$success_percentage)
########################
# PLOT: Main Category  #
data <- data %>%
mutate(bi_cat = ifelse(state == "successful", 1, 0))
p5 <- data %>%
ggplot(aes(x = main_category, y = usd_goal_real, group = main_category)) + #We transform the fill to %
geom_boxplot(outlier.shape = NA) +
coord_cartesian(ylim = c(0, 100000)) +
scale_fill_paletteer_c("grDevices::BluGrn", direction = -1) +
xlab(element_blank()) +
ylab("Project funding goal ($)") +
ggtitle("Funding Goal Distribution by Categories") +
theme_fivethirtyeight() +
theme(axis.text.x = element_text(angle = 60, size = 10, vjust = 1, hjust = 0.8),
axis.title.y = element_text(vjust = 3),
panel.grid.major.y = element_line())
p5
data$main_category <- as.numeric(factor(data$main_category)) # Main Category
data$category <- as.numeric(factor(data$category)) # Category
data$country <- as.numeric(factor(data$country)) # Country
usa_data$main_category <- as.numeric(factor(usa_data$main_category)) # Main Category
usa_data$category <- as.numeric(factor(usa_data$category)) # Category
usa_data$country <- as.numeric(factor(usa_data$country)) # Country
# create a column with the number of day between two column with two different date
data$days_between <- as.numeric(difftime(data$deadline, data$launched, units = "days"))
usa_data$days_between <- as.numeric(difftime(usa_data$deadline, usa_data$launched, units = "days"))
######################
cols <- c("main_category", "usd_goal_real", "launch_year", "category", "country", "days_between", "state_bi")
new_data <- data[cols]
Y <- data["state_bi"]
new_usa_data <- usa_data[cols]
Y_usa <- new_usa_data[, 7]
X_usa <- new_usa_data[, -7]
summary(Y_usa)
set.seed(23) # Set the seed for reproducibility
test_proportion <- 0.3 # (e.g., 70% training, 30% testing)
num_test_samples <- round(nrow(new_data) * test_proportion) # Samples for the test set based on the proportion
test_indices <- sample(nrow(new_data), num_test_samples) # Randomly select the row indices for the test set
# Create the training and test sets for features (X) using indexing
X_train <- new_data[-test_indices, -7]
X_test <- new_data[test_indices, -7]
# Create the training and test sets for the target variable (Y)
Y_train <- new_data[-test_indices, 7]
Y_test <- new_data[test_indices, 7]
# Print the dimensions of the training and test sets
sprintf("X_train: %d Y_train: %d", dim(X_train)[1], length(Y_train))
sprintf("X_test: %d Y_test: %d", dim(X_test)[1], length(Y_test))
plot_accuracy <- function(probabilities, Y, title) {
predictions <- ifelse(probabilities > 0.6, 1, 0)
matrix <- table(Y, predictions)
if (dim(matrix)[2] == 1){
matrix <- cbind(matrix, c(0, 0))
}
print(matrix)
accuracy <- ( matrix[[1,1]] + matrix[[2,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
precision <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[1,2]] )
recall <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[2,1]] )
err_rate <- ( matrix[[2,1]] + matrix[[1,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
fn_rate <- matrix[[1,2]] / ( matrix[[1,2]] + matrix[[2,2]] )
# Show results
print(paste("Err rate: ", err_rate))
print(paste("FN rate: ", fn_rate))
print(paste("Accuracy: ", accuracy))
print(paste("Recall: ", recall))
print(paste("Precision: ", precision))
thresholds <- seq(0.1, 1, by = 0.1)  # Imposta le soglie da 0 a 1 con incrementi di 0.1
accuracy <- numeric(length(thresholds))
recall <- numeric(length(thresholds))
precision <- numeric(length(thresholds))
for (i in 1:length(thresholds)) {
predicted_labels <- ifelse(probabilities > thresholds[i], 1, 0)
matrix <- table(Y, predicted_labels)
if (dim(matrix)[2] == 1){
matrix <- cbind(matrix, c(0, 0))
}
accuracy[i] <- ( matrix[[1,1]] + matrix[[2,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
precision[i] <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[1,2]] )
recall[i] <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[2,1]] )
}
accuracy_df <- data.frame(Threshold = thresholds, Accuracy = accuracy)
acc <- ggplot(accuracy_df, aes(x = Threshold, y = Accuracy)) +
geom_line() +
geom_point() +
labs(x = "Threshold", y = "Accuracy", title = "Accuracy Curve")
graph_df <- data.frame(Threshold = thresholds, Recall = recall, Precision = precision)
graph_df <- graph_df %>% filter(!is.na(Precision))
prec_rec <- ggplot(graph_df, aes(x = Threshold)) +
geom_line(aes(y = Recall)) +
geom_line(aes(y = Precision)) +
geom_point(aes(y = Recall)) +
geom_point(aes(y = Precision)) +
labs(title = "Precision & Recall",
x = "Threshold",
y = "Precision & Recall") +
guides(color = guide_legend(title = "Curve", labels = c("Recall", "Precision")))
grid.arrange(acc, prec_rec, nrow = 1)
roc.out <- roc(Y, probabilities, levels=c(0, 1))
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE,
xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
}
# USA TEST FUNCTION
usa_test_function <- function(model, threshold) {
predictions_usa <- predict(model, newdata = new_usa_data[, -8], type = "response") # Apply logistic regression to the test set
predicted_labels <- ifelse(predictions_usa > threshold, 1, 0) # Convert predicted probabilities to class labels
accuracy <- sum(predicted_labels == Y_usa) / length(Y_usa) # Calculate accuracy
print(accuracy) # Print the accuracy
plot_accuracy(predicted_labels, Y_usa, "USA: Simple LogReg")
}
train_balanced <- ovun.sample(state_bi ~ ., data = new_data, method = "both", N = 17049, seed = 1)$data
prop.table(table(train_balanced$state_bi))
head(train_balanced)
test_proportion <- 0.3 # (e.g., 70% training, 30% testing)
num_test_samples <- round(nrow(train_balanced) * test_proportion) # Samples for the test set based on the proportion
test_indices <- sample(nrow(train_balanced), num_test_samples) # Randomly select the row indices for the test set
# Create the training and test sets for features (X) using indexing
X_train_bal <- train_balanced[-test_indices, -7]
X_test_bal <- train_balanced[test_indices, -7]
# Create the training and test sets for the target variable (Y)
Y_train_bal <- train_balanced[-test_indices, 7]
Y_test_bal <- train_balanced[test_indices, 7]
prop.table(table(Y_train_bal))
prop.table(table(Y_test_bal))
rf_model <- randomForest(X_train, Y_train)
summary(rf_model)
probabilities <- predict(rf_model, newdata = X_usa, type = "prob")
plot_accuracy(probabilities[, 2], Y_usa, "Random Forest Balanced")
rf_model <- randomForest(X_train_bal, Y_train_bal)
probabilities <- predict(rf_model, newdata = X_usa, type = "prob")
plot_accuracy(probabilities[, 2], Y_usa, "Random Forest Balanced")
data <- read.csv('ks-projects-201801.csv', header = T, na.string='')
summary(data)
head(data, 2)
anyNA(data)
colSums(is.na(data))
sum(data$country == 'N,0"')
identical(which(data$country == 'N,0"'), which(is.na(data$usd.pledged)))
data <- data %>%
filter(state %in% c("failed", "canceled", "successful", "suspended")) %>% #Omitting live and undefined projects
mutate(state_bi = factor(ifelse(state == "successful", 1, 0))) %>% #Creating a factor -> 1 for a successful project, 0 otherwise
mutate(launch_year = year(launched)) %>% #We aren't interested in datetime in our case, just the year of the project
mutate(launched = date(launched)) #We are only interested in the date, as datetime is too detailed for our uses
library(gridExtra)
library("ROSE")
library(car)
library(lubridate)
library(glmnet)
library(outliers)
library(corrplot)
library(dplyr)
library(ggplot2)
library(MASS)
library(randomForest)
library(pROC)
library(paletteer)
library(ggthemes)
data <- data %>%
filter(state %in% c("failed", "canceled", "successful", "suspended")) %>% #Omitting live and undefined projects
mutate(state_bi = factor(ifelse(state == "successful", 1, 0))) %>% #Creating a factor -> 1 for a successful project, 0 otherwise
mutate(launch_year = year(launched)) %>% #We aren't interested in datetime in our case, just the year of the project
mutate(launched = date(launched)) #We are only interested in the date, as datetime is too detailed for our uses
europe <- c("DE", "FR", "NL", "IT", "ES", "SE", "DK", "IE", "CH", "NO", "BE", "AT", "LU")
usa_data <- data[data$country == "US", ]
data <- data %>%
filter(!(year(launched) %in% c(1970, 2018))) %>% #We remove the projects from 1970 and 2018
filter(!(country == 'N,0"')) %>% #We remove the projects with missing information about the countries
filter(!is.na(name)) %>% #Remove 4 row with name isNa
filter(country %in% europe) %>% #Remove the country not in EU
filter(usd_goal_real > 1000 & usd_goal_real < 100000)
usa_data <- usa_data %>%
filter(!(year(launched) %in% c(1970, 2018))) %>% #We remove the projects from 1970 and 2018
filter(!(country == 'N,0"')) %>% #We remove the projects with missing information about the countries
filter(!is.na(name)) %>% #Remove 4 row with name isNa
filter(usd_goal_real > 1000 & usd_goal_real < 100000)
#######################
data$main_category <- as.numeric(factor(data$main_category)) # Main Category
data$category <- as.numeric(factor(data$category)) # Category
data$country <- as.numeric(factor(data$country)) # Country
usa_data$main_category <- as.numeric(factor(usa_data$main_category)) # Main Category
usa_data$category <- as.numeric(factor(usa_data$category)) # Category
usa_data$country <- as.numeric(factor(usa_data$country)) # Country
# create a column with the number of day between two column with two different date
data$days_between <- as.numeric(difftime(data$deadline, data$launched, units = "days"))
usa_data$days_between <- as.numeric(difftime(usa_data$deadline, usa_data$launched, units = "days"))
cols <- c("main_category", "usd_goal_real", "launch_year", "category", "country", "days_between", "state_bi")
new_data <- data[cols]
Y <- data["state_bi"]
new_usa_data <- usa_data[cols]
Y_usa <- new_usa_data[, 7]
X_usa <- new_usa_data[, -7]
#new_data <- usa_data[cols]
#Y <- new_data["state_bi"]
summary(Y_usa)
set.seed(23) # Set the seed for reproducibility
test_proportion <- 0.3 # (e.g., 70% training, 30% testing)
num_test_samples <- round(nrow(new_data) * test_proportion) # Samples for the test set based on the proportion
test_indices <- sample(nrow(new_data), num_test_samples) # Randomly select the row indices for the test set
# Create the training and test sets for features (X) using indexing
X_train <- new_data[-test_indices, -7]
X_test <- new_data[test_indices, -7]
# Create the training and test sets for the target variable (Y)
Y_train <- new_data[-test_indices, 7]
Y_test <- new_data[test_indices, 7]
# Print the dimensions of the training and test sets
sprintf("X_train: %d Y_train: %d", dim(X_train)[1], length(Y_train))
sprintf("X_test: %d Y_test: %d", dim(X_test)[1], length(Y_test))
plot_accuracy <- function(probabilities, Y, title) {
predictions <- ifelse(probabilities > 0.6, 1, 0)
matrix <- table(Y, predictions)
if (dim(matrix)[2] == 1){
matrix <- cbind(matrix, c(0, 0))
}
print(matrix)
accuracy <- ( matrix[[1,1]] + matrix[[2,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
precision <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[1,2]] )
recall <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[2,1]] )
err_rate <- ( matrix[[2,1]] + matrix[[1,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
fn_rate <- matrix[[1,2]] / ( matrix[[1,2]] + matrix[[2,2]] )
# Show results
print(paste("Err rate: ", err_rate))
print(paste("FN rate: ", fn_rate))
print(paste("Accuracy: ", accuracy))
print(paste("Recall: ", recall))
print(paste("Precision: ", precision))
thresholds <- seq(0.1, 1, by = 0.1)  # Imposta le soglie da 0 a 1 con incrementi di 0.1
accuracy <- numeric(length(thresholds))
recall <- numeric(length(thresholds))
precision <- numeric(length(thresholds))
for (i in 1:length(thresholds)) {
predicted_labels <- ifelse(probabilities > thresholds[i], 1, 0)
matrix <- table(Y, predicted_labels)
if (dim(matrix)[2] == 1){
matrix <- cbind(matrix, c(0, 0))
}
accuracy[i] <- ( matrix[[1,1]] + matrix[[2,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
precision[i] <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[1,2]] )
recall[i] <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[2,1]] )
}
accuracy_df <- data.frame(Threshold = thresholds, Accuracy = accuracy)
acc <- ggplot(accuracy_df, aes(x = Threshold, y = Accuracy)) +
geom_line() +
geom_point() +
labs(x = "Threshold", y = "Accuracy", title = "Accuracy Curve")
graph_df <- data.frame(Threshold = thresholds, Recall = recall, Precision = precision)
graph_df <- graph_df %>% filter(!is.na(Precision))
prec_rec <- ggplot(graph_df, aes(x = Threshold)) +
geom_line(aes(y = Recall)) +
geom_line(aes(y = Precision)) +
geom_point(aes(y = Recall)) +
geom_point(aes(y = Precision)) +
labs(title = "Precision & Recall",
x = "Threshold",
y = "Precision & Recall") +
guides(color = guide_legend(title = "Curve", labels = c("Recall", "Precision")))
grid.arrange(acc, prec_rec, nrow = 1)
roc.out <- roc(Y, probabilities, levels=c(0, 1))
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE,
xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
}
logreg_model <- glm(Y_train ~ ., data = X_train, family = binomial)
summary(logreg_model) # Print the summary of the model
s<- summary(logreg_model)
r2 <- 1 - (s$deviance/s$null.deviance)
1/(1-r2)
vif(logreg_model) # VIF
# There aren't columns with VIF > 1/(1 - r)
pred_logreg_model <- predict(logreg_model, newdata = X_test, type = "response") # Apply logistic regression to the test set
hist(pred_logreg_model)
predicted_labels <- ifelse(pred_logreg_model > 0.5, 1, 0) # Convert predicted probabilities to class labels
plot_accuracy(pred_logreg_model, Y_test, "Simple LogReg")
summary(logreg_model)
stepwise_model <- stepAIC(glm(Y_train ~ ., data = X_train, family = binomial), direction = "both", trace = FALSE)
summary(stepwise_model) # Print the summary of the model
logreg_model <- glm(Y_train ~ ., data = X_train, family = binomial)
summary(logreg_model) # Print the summary of the model
s<- summary(logreg_model)
r2 <- 1 - (s$deviance/s$null.deviance)
1/(1-r2)
vif(logreg_model) # VIF
# There aren't columns with VIF > 1/(1 - r)
pred_logreg_model <- predict(logreg_model, newdata = X_test, type = "response") # Apply logistic regression to the test set
hist(pred_logreg_model)
predicted_labels <- ifelse(pred_logreg_model > 0.5, 1, 0) # Convert predicted probabilities to class labels
plot_accuracy(pred_logreg_model, Y_test, "Simple LogReg")
prop.table(table(train_balanced$state_bi))
prop.table(table(Y_test_bal))
prop.table(table(Y_train_bal))
train_balanced.shape
table(train_balanced$state_bi)
table(new_data$state_bi)
shapiro.test(sample$main_category[sample$Y_train_bal == 0])
shapiro.test(sample$main_category[sample$Y_train_bal == 1])
library(class)
library(gridExtra)
library("ROSE")
library(car)
library(lubridate)
library(glmnet)
library(outliers)
library(corrplot)
library(dplyr)
library(ggplot2)
library(MASS)
library(randomForest)
library(pROC)
library(paletteer)
library(ggthemes)
library(class)
data <- read.csv('ks-projects-201801.csv', header = T, na.string='')
summary(data)
head(data, 2)
#################################
anyNA(data)
colSums(is.na(data))
sum(data$country == 'N,0"')
identical(which(data$country == 'N,0"'), which(is.na(data$usd.pledged)))
data <- data %>%
filter(state %in% c("failed", "canceled", "successful", "suspended")) %>% #Omitting live and undefined projects
mutate(state_bi = factor(ifelse(state == "successful", 1, 0))) %>% #Creating a factor -> 1 for a successful project, 0 otherwise
mutate(launch_year = year(launched)) %>% #We aren't interested in datetime in our case, just the year of the project
mutate(launched = date(launched)) #We are only interested in the date, as datetime is too detailed for our uses
europe <- c("DE", "FR", "NL", "IT", "ES", "SE", "DK", "IE", "CH", "NO", "BE", "AT", "LU")
usa_data <- data[data$country == "US", ]
data <- data %>%
filter(!(year(launched) %in% c(1970, 2018))) %>% #We remove the projects from 1970 and 2018
filter(!(country == 'N,0"')) %>% #We remove the projects with missing information about the countries
filter(!is.na(name)) %>% #Remove 4 row with name isNa
filter(country %in% europe) %>% #Remove the country not in EU
filter(usd_goal_real > 1000 & usd_goal_real < 100000)
usa_data <- usa_data %>%
filter(!(year(launched) %in% c(1970, 2018))) %>% #We remove the projects from 1970 and 2018
filter(!(country == 'N,0"')) %>% #We remove the projects with missing information about the countries
filter(!is.na(name)) %>% #Remove 4 row with name isNa
filter(usd_goal_real > 1000 & usd_goal_real < 100000)
#######################
data$main_category <- as.numeric(factor(data$main_category)) # Main Category
data$category <- as.numeric(factor(data$category)) # Category
data$country <- as.numeric(factor(data$country)) # Country
usa_data$main_category <- as.numeric(factor(usa_data$main_category)) # Main Category
usa_data$category <- as.numeric(factor(usa_data$category)) # Category
usa_data$country <- as.numeric(factor(usa_data$country)) # Country
# create a column with the number of day between two column with two different date
data$days_between <- as.numeric(difftime(data$deadline, data$launched, units = "days"))
usa_data$days_between <- as.numeric(difftime(usa_data$deadline, usa_data$launched, units = "days"))
cols <- c("main_category", "usd_goal_real", "category", "country", "launch_year", "days_between")
data_correlation <- data[cols]
new_data_num <- data.frame(lapply(data_correlation, as.integer))
cor_matrix <- cor(new_data_num)
corrplot(cor_matrix, method = "number", col = colorRampPalette(c("white", "black"))(100))
######################
# Correlation Matrix #
######################
# NON METTERE LE CATEGORICAL VARIABILI NELLA MATRICE
cols <- c("main_category", "usd_goal_real", "category", "country", "launch_year", "days_between")
cols <- c("main_category", "usd_goal_real", "launch_year", "category", "country", "days_between", "state_bi")
new_data <- data[cols]
Y <- data["state_bi"]
new_usa_data <- usa_data[cols]
Y_usa <- new_usa_data[, 7]
X_usa <- new_usa_data[, -7]
summary(Y_usa)
set.seed(23) # Set the seed for reproducibility
test_proportion <- 0.3 # (e.g., 70% training, 30% testing)
num_test_samples <- round(nrow(new_data) * test_proportion) # Samples for the test set based on the proportion
test_indices <- sample(nrow(new_data), num_test_samples) # Randomly select the row indices for the test set
# Create the training and test sets for features (X) using indexing
X_train <- new_data[-test_indices, -7]
X_test <- new_data[test_indices, -7]
# Create the training and test sets for the target variable (Y)
Y_train <- new_data[-test_indices, 7]
Y_test <- new_data[test_indices, 7]
# Print the dimensions of the training and test sets
sprintf("X_train: %d Y_train: %d", dim(X_train)[1], length(Y_train))
sprintf("X_test: %d Y_test: %d", dim(X_test)[1], length(Y_test))
plot_accuracy <- function(probabilities, Y, title) {
predictions <- ifelse(probabilities > 0.6, 1, 0)
matrix <- table(Y, predictions)
if (dim(matrix)[2] == 1){
matrix <- cbind(matrix, c(0, 0))
}
print(matrix)
accuracy <- ( matrix[[1,1]] + matrix[[2,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
precision <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[1,2]] )
recall <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[2,1]] )
err_rate <- ( matrix[[2,1]] + matrix[[1,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
fn_rate <- matrix[[1,2]] / ( matrix[[1,2]] + matrix[[2,2]] )
# Show results
print(paste("Err rate: ", err_rate))
print(paste("FN rate: ", fn_rate))
print(paste("Accuracy: ", accuracy))
print(paste("Recall: ", recall))
print(paste("Precision: ", precision))
thresholds <- seq(0.1, 1, by = 0.1)  # Imposta le soglie da 0 a 1 con incrementi di 0.1
accuracy <- numeric(length(thresholds))
recall <- numeric(length(thresholds))
precision <- numeric(length(thresholds))
for (i in 1:length(thresholds)) {
predicted_labels <- ifelse(probabilities > thresholds[i], 1, 0)
matrix <- table(Y, predicted_labels)
if (dim(matrix)[2] == 1){
matrix <- cbind(matrix, c(0, 0))
}
accuracy[i] <- ( matrix[[1,1]] + matrix[[2,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
precision[i] <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[1,2]] )
recall[i] <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[2,1]] )
}
accuracy_df <- data.frame(Threshold = thresholds, Accuracy = accuracy)
acc <- ggplot(accuracy_df, aes(x = Threshold, y = Accuracy)) +
geom_line() +
geom_point() +
labs(x = "Threshold", y = "Accuracy", title = "Accuracy Curve")
graph_df <- data.frame(Threshold = thresholds, Recall = recall, Precision = precision)
graph_df <- graph_df %>% filter(!is.na(Precision))
prec_rec <- ggplot(graph_df, aes(x = Threshold)) +
geom_line(aes(y = Recall)) +
geom_line(aes(y = Precision)) +
geom_point(aes(y = Recall)) +
geom_point(aes(y = Precision)) +
labs(title = "Precision & Recall",
x = "Threshold",
y = "Precision & Recall") +
guides(color = guide_legend(title = "Curve", labels = c("Recall", "Precision")))
grid.arrange(acc, prec_rec, nrow = 1)
roc.out <- roc(Y, probabilities, levels=c(0, 1))
plot(roc.out, print.auc=TRUE, legacy.axes=TRUE,
xlab="False positive rate", ylab="True positive rate")
auc(roc.out)
}
# USA TEST FUNCTION
usa_test_function <- function(model, threshold) {
predictions_usa <- predict(model, newdata = new_usa_data[, -8], type = "response") # Apply logistic regression to the test set
predicted_labels <- ifelse(predictions_usa > threshold, 1, 0) # Convert predicted probabilities to class labels
accuracy <- sum(predicted_labels == Y_usa) / length(Y_usa) # Calculate accuracy
print(accuracy) # Print the accuracy
plot_accuracy(predicted_labels, Y_usa, "USA: Simple LogReg")
}
logreg_model <- glm(Y_train ~ ., data = X_train, family = binomial)
summary(logreg_model) # Print the summary of the model
s<- summary(logreg_model)
r2 <- 1 - (s$deviance/s$null.deviance)
1/(1-r2)
vif(logreg_model) # VIF
# There aren't columns with VIF > 1/(1 - r)
pred_logreg_model <- predict(logreg_model, newdata = X_test, type = "response") # Apply logistic regression to the test set
hist(pred_logreg_model)
predicted_labels <- ifelse(pred_logreg_model > 0.5, 1, 0) # Convert predicted probabilities to class labels
plot_accuracy(pred_logreg_model, Y_test, "Simple LogReg")
summary(logreg_model)
# K-NN #
########
knn_predictions <- knn(train = X_train, test = X_train, cl = Y_train, k = k)
knn_predictions <- knn(train = X_train, test = X_train, cl = Y_train, k = 3)
print(knn_predictions)
########
# K-NN #
########
knn_predictions <- knn(train = X_train, test = X_train, cl = Y_train, k = 3, prob=TRUE)
print(knn_predictions)
matrix <- table(Y, knn_predictions)
########
# K-NN #
########
knn_predictions <- knn(train = X_train, test = X_test, cl = Y_train, k = 3, prob=TRUE)
matrix <- table(Y_test, knn_predictions)
if (dim(matrix)[2] == 1){
matrix <- cbind(matrix, c(0, 0))
}
print(matrix)
########
# K-NN #
########
knn_predictions <- knn(train = X_train, test = X_test, cl = Y_train, k = 3)
matrix <- table(Y_test, knn_predictions)
if (dim(matrix)[2] == 1){
matrix <- cbind(matrix, c(0, 0))
}
print(matrix)
accuracy <- ( matrix[[1,1]] + matrix[[2,2]] ) / ( matrix[[1,1]] + matrix[[1,2]] + matrix[[2,1]] + matrix[[2,2]] )
precision <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[1,2]] )
recall <- matrix[[2,2]] / ( matrix[[2,2]] + matrix[[2,1]] )
print(paste("Accuracy: ", accuracy))
print(paste("Recall: ", recall))
print(paste("Precision: ", precision))
